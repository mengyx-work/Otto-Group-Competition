{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split the training data\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "X = train_df.values.copy()\n",
    "test_X = test_df.values.copy()\n",
    "np.random.shuffle(X)\n",
    "X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels).astype(np.int32)\n",
    "\n",
    "### we need a test set that we didn't train on to find the best weights for combining the classifiers\n",
    "sss = StratifiedShuffleSplit(y, 1, test_size=0.1, random_state=1234)\n",
    "\n",
    "for train_index, test_index in sss:\n",
    "    print 'split the training data'\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "train_X, train_y = X[train_index], y[train_index]\n",
    "check_X, check_y = X[test_index], y[test_index]\n",
    "\n",
    "\n",
    "test_X, test_ids = test_X[:, 1:].astype(np.float32), test_X[:, 0].astype(str)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  (55690, 93)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'training data: ', train_X.shape\n",
    "print \n",
    "\n",
    "num_classes = len(encoder.classes_)\n",
    "num_features = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DropoutLayer      \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.908080\u001b[0m  |  \u001b[32m  0.697891\u001b[0m  |     1.301178  |     72.47%  |  1.0s\n",
      "     2  |  \u001b[94m  0.686592\u001b[0m  |  \u001b[32m  0.636604\u001b[0m  |     1.078522  |     75.40%  |  1.0s\n",
      "     3  |  \u001b[94m  0.649086\u001b[0m  |  \u001b[32m  0.608596\u001b[0m  |     1.066530  |     76.54%  |  1.0s\n",
      "     4  |  \u001b[94m  0.623866\u001b[0m  |  \u001b[32m  0.594349\u001b[0m  |     1.049662  |     76.76%  |  1.0s\n",
      "     5  |  \u001b[94m  0.608625\u001b[0m  |  \u001b[32m  0.583194\u001b[0m  |     1.043606  |     77.44%  |  1.0s\n",
      "     6  |  \u001b[94m  0.593090\u001b[0m  |  \u001b[32m  0.572651\u001b[0m  |     1.035693  |     77.57%  |  1.1s\n",
      "     7  |  \u001b[94m  0.583419\u001b[0m  |  \u001b[32m  0.564730\u001b[0m  |     1.033093  |     78.07%  |  1.0s\n",
      "     8  |  \u001b[94m  0.570828\u001b[0m  |  \u001b[32m  0.559696\u001b[0m  |     1.019890  |     78.26%  |  1.1s\n",
      "     9  |  \u001b[94m  0.564361\u001b[0m  |  \u001b[32m  0.558933\u001b[0m  |     1.009712  |     78.07%  |  1.0s\n",
      "    10  |  \u001b[94m  0.558847\u001b[0m  |  \u001b[32m  0.553606\u001b[0m  |     1.009466  |     78.39%  |  1.0s\n",
      "    11  |  \u001b[94m  0.551200\u001b[0m  |  \u001b[32m  0.548170\u001b[0m  |     1.005527  |     78.48%  |  1.0s\n",
      "    12  |  \u001b[94m  0.544155\u001b[0m  |  \u001b[32m  0.545625\u001b[0m  |     0.997306  |     78.65%  |  1.0s\n",
      "    13  |  \u001b[94m  0.540447\u001b[0m  |  \u001b[32m  0.544595\u001b[0m  |     0.992384  |     78.52%  |  1.0s\n",
      "    14  |  \u001b[94m  0.536965\u001b[0m  |  \u001b[32m  0.542338\u001b[0m  |     0.990093  |     78.67%  |  1.0s\n",
      "    15  |  \u001b[94m  0.531994\u001b[0m  |  \u001b[32m  0.541478\u001b[0m  |     0.982485  |     79.02%  |  1.0s\n",
      "    16  |  \u001b[94m  0.529197\u001b[0m  |  \u001b[32m  0.539254\u001b[0m  |     0.981351  |     78.60%  |  1.0s\n",
      "    17  |  \u001b[94m  0.521111\u001b[0m  |  \u001b[32m  0.535580\u001b[0m  |     0.972985  |     79.01%  |  1.0s\n",
      "    18  |  \u001b[94m  0.519033\u001b[0m  |  \u001b[32m  0.534843\u001b[0m  |     0.970441  |     78.95%  |  1.0s\n",
      "    19  |  \u001b[94m  0.515051\u001b[0m  |  \u001b[32m  0.531622\u001b[0m  |     0.968829  |     78.99%  |  1.0s\n",
      "    20  |  \u001b[94m  0.512978\u001b[0m  |  \u001b[32m  0.531385\u001b[0m  |     0.965359  |     79.04%  |  1.0s\n",
      "    21  |  \u001b[94m  0.509476\u001b[0m  |    0.536412  |     0.949784  |     78.69%  |  1.0s\n",
      "    22  |  \u001b[94m  0.506954\u001b[0m  |  \u001b[32m  0.528605\u001b[0m  |     0.959042  |     79.17%  |  1.0s\n",
      "    23  |  \u001b[94m  0.502105\u001b[0m  |    0.530127  |     0.947140  |     79.08%  |  1.0s\n",
      "    24  |    0.502569  |  \u001b[32m  0.527684\u001b[0m  |     0.952406  |     79.03%  |  1.0s\n",
      "    25  |  \u001b[94m  0.499462\u001b[0m  |  \u001b[32m  0.526913\u001b[0m  |     0.947903  |     79.11%  |  1.0s\n",
      "    26  |  \u001b[94m  0.498057\u001b[0m  |  \u001b[32m  0.526274\u001b[0m  |     0.946384  |     79.28%  |  1.0s\n",
      "    27  |  \u001b[94m  0.493751\u001b[0m  |  \u001b[32m  0.523964\u001b[0m  |     0.942338  |     79.34%  |  1.0s\n",
      "    28  |  \u001b[94m  0.489914\u001b[0m  |    0.527109  |     0.929436  |     79.15%  |  1.0s\n",
      "    29  |  \u001b[94m  0.489194\u001b[0m  |    0.524892  |     0.931989  |     79.60%  |  1.0s\n",
      "    30  |  \u001b[94m  0.486064\u001b[0m  |  \u001b[32m  0.520395\u001b[0m  |     0.934028  |     79.64%  |  1.0s\n",
      "    31  |  \u001b[94m  0.481813\u001b[0m  |    0.521634  |     0.923662  |     79.63%  |  1.1s\n",
      "    32  |    0.483520  |    0.530906  |     0.910744  |     79.07%  |  1.0s\n",
      "    33  |    0.485073  |    0.521674  |     0.929840  |     79.77%  |  1.0s\n",
      "    34  |  \u001b[94m  0.479564\u001b[0m  |    0.523622  |     0.915859  |     79.75%  |  1.0s\n",
      "    35  |  \u001b[94m  0.476345\u001b[0m  |    0.522172  |     0.912236  |     79.64%  |  1.0s\n",
      "    36  |  \u001b[94m  0.475413\u001b[0m  |    0.520694  |     0.913037  |     79.70%  |  1.0s\n",
      "    37  |  \u001b[94m  0.474076\u001b[0m  |    0.520440  |     0.910915  |     79.48%  |  1.0s\n",
      "    38  |  \u001b[94m  0.471147\u001b[0m  |    0.520564  |     0.905071  |     79.70%  |  1.0s\n",
      "    39  |  \u001b[94m  0.468799\u001b[0m  |    0.520999  |     0.899807  |     79.53%  |  1.1s\n",
      "    40  |    0.468965  |  \u001b[32m  0.518202\u001b[0m  |     0.904984  |     79.60%  |  1.0s\n",
      "    41  |  \u001b[94m  0.468689\u001b[0m  |    0.519920  |     0.901464  |     79.44%  |  1.0s\n",
      "    42  |  \u001b[94m  0.461577\u001b[0m  |    0.522905  |     0.882717  |     79.70%  |  1.0s\n",
      "    43  |    0.464727  |    0.519981  |     0.893740  |     79.64%  |  1.0s\n",
      "    44  |    0.463546  |    0.518744  |     0.893594  |     79.93%  |  1.0s\n",
      "    45  |    0.463389  |    0.519477  |     0.892030  |     79.90%  |  1.0s\n",
      "    46  |  \u001b[94m  0.459929\u001b[0m  |  \u001b[32m  0.518115\u001b[0m  |     0.887696  |     79.73%  |  1.0s\n",
      "    47  |    0.460088  |  \u001b[32m  0.518062\u001b[0m  |     0.888095  |     79.93%  |  1.0s\n",
      "    48  |  \u001b[94m  0.458149\u001b[0m  |  \u001b[32m  0.517559\u001b[0m  |     0.885212  |     79.98%  |  1.0s\n",
      "    49  |  \u001b[94m  0.457741\u001b[0m  |    0.521049  |     0.878498  |     80.05%  |  1.0s\n",
      "    50  |  \u001b[94m  0.453594\u001b[0m  |  \u001b[32m  0.517515\u001b[0m  |     0.876484  |     79.95%  |  1.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x10766d668>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x107e68b50>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x107e68b10>,\n",
       "     dense0_num_units=200, dense1_num_units=200, dropout_p=0.4,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x107d90de8>, max_epochs=50,\n",
       "     more_params={}, on_epoch_finished=(), on_training_finished=(),\n",
       "     output_nonlinearity=<function softmax at 0x107c83848>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x107d70140>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layers0 = [('input', InputLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('output', DenseLayer)]\n",
    "\n",
    "\n",
    "net0 = NeuralNet(layers=layers0,\n",
    "                 \n",
    "                 input_shape=(None, num_features),\n",
    "                 dense0_num_units=200,\n",
    "                 dropout_p=0.4,\n",
    "                 dense1_num_units=200,\n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,\n",
    "                 update_learning_rate=0.01,\n",
    "                 update_momentum=0.9,\n",
    "                 \n",
    "                 eval_size=0.2,\n",
    "                 verbose=1,\n",
    "                 max_epochs=50)\n",
    "\n",
    "\n",
    "net0.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss 0.510064097173\n"
     ]
    }
   ],
   "source": [
    "y_prob = net0.predict_proba(check_X)\n",
    "print('LogLoss {score}'.format(score=log_loss(check_y, y_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(144368, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(y_prob)\n",
    "y_prob.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
